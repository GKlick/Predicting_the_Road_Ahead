{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:21:09.592962Z",
     "start_time": "2019-03-06T23:20:35.972027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "\n",
    "#data viz imports\n",
    "import plotly.graph_objs as go\n",
    "#use this format for working locally \n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot, plot_mpl\n",
    "init_notebook_mode(connected=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#model imports\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import fbprophet\n",
    "\n",
    "#set numbers formatting \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:21:09.640874Z",
     "start_time": "2019-03-06T23:21:09.632778Z"
    }
   },
   "outputs": [],
   "source": [
    "def region_df(region):\n",
    "    \n",
    "    '''\n",
    "    Create a new dataframe from the main traffic_df, sectioning out the regions\n",
    "    \n",
    "    :region: the region (as a number) to create into own data frame\n",
    "\n",
    "    Returns new dataframe with informations only pertaining to region \n",
    "    '''\n",
    "    \n",
    "    #section out region\n",
    "    region_df= traffic_df[traffic_df.REGION_ID == region]\n",
    "    \n",
    "    #resets index\n",
    "    region_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #drop dups. This should have been corrected earlier, this is just a fail safe\n",
    "    region_df.drop_duplicates(inplace= True)\n",
    "    \n",
    "    #remove data that does not pertain to local roads (over 40mph)\n",
    "    #and observations where no cars are tracked (0mph)\n",
    "    region_df = region_df[(region_df.SPEED < 40) & (region_df.SPEED != 0)]\n",
    "    \n",
    "    #return new data frame\n",
    "    return region_df\n",
    "\n",
    "def train_test_ts(df, holdout = .1):\n",
    "    \n",
    "    '''\n",
    "    Given a time series dataframe, split into train and test\n",
    "    \n",
    "    :df: time series dataframe\n",
    "    :holdout: The percent size for the test data\n",
    "\n",
    "    Returns the train and test dataframes\n",
    "    '''\n",
    "    \n",
    "    train_size = int(len(df) * (1-holdout))\n",
    "    train, test = df[0:train_size], df[train_size:]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:21:09.687405Z",
     "start_time": "2019-03-06T23:21:09.669804Z"
    }
   },
   "outputs": [],
   "source": [
    "def Aug_Dicky_Fuller(column,stationary_count):\n",
    "    '''\n",
    "    Given a column of data going to be used for time series modeling,\n",
    "    performs the Augmented Dickey Fuller stationarity test. \n",
    "    If n0 is rejected the data is stationary\n",
    "    \n",
    "    :column: the column of data, as a series or array\n",
    "    :stationary_count: an int that used to track number of stationarity passed\n",
    "\n",
    "    Returns stationary_count, increased by 1 if the data passed\n",
    "    '''\n",
    "    \n",
    "    #null: non stationary data\n",
    "    #alternative: stationary\n",
    "    #reject null if p =< .05\n",
    "    \n",
    "    \n",
    "    #p value\n",
    "    #reject null, meaning data is stationary\n",
    "    if adfuller(column)[1] <= .05:\n",
    "        stationary_count += 1\n",
    "        return stationary_count\n",
    "\n",
    "    #accept null, meaning data has a trend\n",
    "    else:\n",
    "        return stationary_count \n",
    "    \n",
    "def KPSS(column, stationary_count):\n",
    "    '''\n",
    "    Given a column of data going to be used for time series modeling,\n",
    "    performs the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) stationarity test. \n",
    "    If n0 is accepted the data is stationary\n",
    "    \n",
    "    :column: the column of data, as a series or array\n",
    "    :stationary_count: an int that used to track number of stationarity passed\n",
    "\n",
    "    Returns stationary_count, increased by 1 if the data passed\n",
    "    '''\n",
    "    \n",
    "    #Null: stationary data\n",
    "    #alternative: trend\n",
    "    \n",
    "    #reject null, meaning data has trend\n",
    "    if kpss(column)[1] <= .05:\n",
    "        return stationary_count \n",
    "    \n",
    "    #accept null, meaning data does not have a trend\n",
    "    else:\n",
    "        stationary_count += 1\n",
    "        return stationary_count\n",
    "\n",
    "def mean_std_check(column, stationary_count):\n",
    "    '''\n",
    "    Given a column of data going to be used for time series modeling,\n",
    "    split data in half and check the mean and standard deviation of each half.\n",
    "    \n",
    "    This method was obtained through some blog posts, it is not as robust as\n",
    "    the previous methods\n",
    "    \n",
    "    :column: the column of data, as a series or array\n",
    "    :stationary_count: an int that used to track number of stationarity passed\n",
    "\n",
    "    Returns stationary_count, increased by 1 if the data passed\n",
    "    '''\n",
    "\n",
    "    #split data into two parts, check mean and standard deviation\n",
    "    first_half_mean = round(column[:(len(column)//2)].mean())\n",
    "    second_half_mean = round(column[(len(column)//2):].mean())\n",
    "\n",
    "    first_half_std = round(column[:(len(column)//2)].std())\n",
    "    second_half_std = round(column[(len(column)//2):].std())\n",
    "\n",
    "    #check if the mean is constant over time\n",
    "    if first_half_mean == second_half_mean:\n",
    "        stationary_count += 1\n",
    "        return stationary_count\n",
    "\n",
    "    #else if the stand deviation is constant throughout the observations,\n",
    "    #and the means are within a standard deviation, then it is stationary\n",
    "    elif first_half_std == second_half_std:\n",
    "        if abs(first_half_mean - second_half_mean) <= first_half_std:\n",
    "            stationary_count += 1\n",
    "            return stationary_count\n",
    "        else:\n",
    "            return stationary_count\n",
    "    else:\n",
    "        return stationary_count\n",
    "\n",
    "def check_stationarity(column):    \n",
    "    '''\n",
    "    Given a column of data going to be used for time series modeling,\n",
    "    performs 3 stationarity tests to check trend over time. \n",
    "    Returns True if 2 or more tests show stationarity in the data\n",
    "    \n",
    "    :column: the column of data, as a series or array\n",
    "\n",
    "    Returns True if data passes at least two stationary test, else returns False\n",
    "    '''\n",
    "\n",
    "    \n",
    "    #an in variable that will be fed to each tests, increasing by one for every test it passes\n",
    "    stationary_count = 0\n",
    "\n",
    "    #perform 3 tests to check for stationary, take the majority vote\n",
    "    #each tests returns the stationary_count, increasing it by one if the data passes the test\n",
    "    if mean_std_check(column,KPSS(column, Aug_Dicky_Fuller(column,stationary_count))) >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def forcast_error_metrics(dataframe,forecast):    \n",
    "    '''\n",
    "    Given dataframe of actual values and forecasted values, \n",
    "    returns a number of error metrics\n",
    "    \n",
    "    :dataframe: the dataframe with the real values\n",
    "    :forecast: a series with the forecasted values\n",
    "\n",
    "    Returns a number of error metrics based off predictions\n",
    "    '''\n",
    "    \n",
    "    #timeseries database that you fit on\n",
    "    #forecast is your prediction\n",
    "    \n",
    "    #the length of the dataframe\n",
    "    df_len = len(dataframe)\n",
    "    #pull the actual values from the dataframe\n",
    "    y = dataframe.loc[:df_len, 'y'].values\n",
    "    #pull the predicted values from data frame, removing the last value\n",
    "    yhat = forecast.loc[:len(y)-1, 'yhat'].values\n",
    "    \n",
    "    #Forecast Error (or Residual Forecast Error)\n",
    "    #actual minutes predicted\n",
    "    for_err = y - yhat\n",
    "\n",
    "    #Mean Forecast Error (or Forecast Bias) (or MAD)\n",
    "    #the mean of the forecast error (on average, how off are you)\n",
    "    mean_for_error = np.mean(for_err)\n",
    "    #print('Mean Forcast Error (Forecast Bias): ',mean_for_error)\n",
    "    \n",
    "    #Mean Absolute Percent Error\n",
    "    #what percent off are you usually?\n",
    "    MAPE = np.mean(abs(y-yhat)/abs(y)) *100\n",
    "    #print('\\nMean Absolute Percent Error (MAPE): ',MAPE)\n",
    "    \n",
    "    #Mean Absolute Error(MAE)\n",
    "    MAE = np.mean(abs(for_err))\n",
    "    #print('\\nMean Absolute Error(MAE): ',MAE)\n",
    "    \n",
    "    #Mean Squared Error(MSE)\n",
    "    MSE = np.mean(for_err**2)\n",
    "    #print('\\nMean Squared Error(MSE): ',MSE)\n",
    "\n",
    "    \n",
    "    #Root Mean Squared Error(RMSE)\n",
    "    RMSE = sqrt(MSE)\n",
    "    #print('\\nRoot Mean Squared Error(RMSE): ',RMSE)\n",
    "\n",
    "    return mean_for_error,MAPE, RMSE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:12:16.708694Z",
     "start_time": "2019-03-06T23:12:16.700707Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_series_cv(ts_df, splits = [.5, .8]):\n",
    "    '''\n",
    "    Given dataframe to train on, and cross validation splits,\n",
    "    return error metrics of cross validations\n",
    "    \n",
    "    :dataframe: the data to train on\n",
    "    :splits: a list of decimals to split data on\n",
    "\n",
    "    Returns a number of error metrics based off predictions\n",
    "    '''\n",
    "    #cross validation\n",
    "    \n",
    "    error_metrics_list = []\n",
    "\n",
    "    for split in splits:\n",
    "        #split data\n",
    "        train, validate = train_test_ts(ts_df, split)\n",
    "\n",
    "        #fit on training data\n",
    "        #this takes a bit\n",
    "        ts_prophet = fbprophet.Prophet()\n",
    "        ts_prophet.fit(train)\n",
    "\n",
    "        #forcast on train and validate\n",
    "        future = pd.DataFrame(pd.concat((train.ds, validate.ds), axis = 0))\n",
    "        forcast = ts_prophet.predict(future)\n",
    "\n",
    "        #measure error metrics for each cv\n",
    "        error_metrics_list.append(forcast_error_metrics(ts_df, forcast))\n",
    "        \n",
    "    mean_forcast_bias = np.mean([item[0] for item in error_metrics_list])\n",
    "    mean_MAPE = np.mean([item[1] for item in error_metrics_list])\n",
    "    mean_RMSE = np.mean([item[2] for item in error_metrics_list])\n",
    "    \n",
    "    return mean_forcast_bias, mean_MAPE, mean_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:40:56.871396Z",
     "start_time": "2019-03-06T23:40:56.858329Z"
    }
   },
   "outputs": [],
   "source": [
    "def fb_prophet_funct(df, cv_on = False, splits = [.5, .8], hours_predict = 12):\n",
    "    '''\n",
    "    Given dataframe to train on, and cross validation splits,\n",
    "    and future forecast, create model, cv (if turned on), and forecasted\n",
    "    \n",
    "    :dataframe: the data to train on\n",
    "    :cv_on: should cross validations be included, autoset to off to save time\n",
    "    :splits: a list of decimals to split data on\n",
    "    :hours_predict: an int, the number of hours in the future to predict\n",
    "\n",
    "    model, forecast, error, and cross validation results if set to True\n",
    "    '''\n",
    "    \n",
    "    #restructure dataframes to be used for fb prophet\n",
    "    ts = df[['TIME','SPEED']]\n",
    "    ts = ts.rename(columns={'TIME': 'ds', 'SPEED': 'y'})\n",
    "    \n",
    "    #check stationarity, this part should take up to 30 seconds \n",
    "    if check_stationarity(ts.y) == True:\n",
    "        \n",
    "        if cv_on == True:\n",
    "            cv_mean_forcast_bias, cv_mean_MAPE, cv_mean_RMSE = time_series_cv(ts)\n",
    "\n",
    "        #final model\n",
    "        ts_prophet = fbprophet.Prophet()\n",
    "        ts_prophet.fit(ts)\n",
    "        \n",
    "        new_times = pd.DataFrame([ts.ds.max()+(i*timedelta(minutes = 10)) for i in range(2,(10*hours_predict)+1)],columns=['ds'])\n",
    "        #new_times_df = pd.DataFrame(new_times_list,columns=['ds'])\n",
    "        future = pd.DataFrame(pd.concat([ts.ds,new_times.ds], axis = 0))\n",
    "        \n",
    "        forecast = ts_prophet.predict(future)\n",
    "        \n",
    "        mean_forcast_bias, mean_MAPE, mean_RMSE = forcast_error_metrics(ts, forecast)\n",
    "        \n",
    "        if cv_on == True:\n",
    "            return ts_prophet, future, forecast, mean_forcast_bias, mean_MAPE, mean_RMSE,\\\n",
    "                    cv_mean_forcast_bias, cv_mean_MAPE, cv_mean_RMSE\n",
    "        else:\n",
    "            return ts_prophet, future, forecast, mean_forcast_bias, mean_MAPE, mean_RMSE\n",
    "                   \n",
    "    else:\n",
    "        print('Data is not stationary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:42:10.507268Z",
     "start_time": "2019-03-06T23:42:10.198632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUS COUNT</th>\n",
       "      <th>NUMBER OF READS</th>\n",
       "      <th>REGION_ID</th>\n",
       "      <th>SPEED</th>\n",
       "      <th>TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>215</td>\n",
       "      <td>5</td>\n",
       "      <td>25.230</td>\n",
       "      <td>2017-01-01 00:10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>230</td>\n",
       "      <td>6</td>\n",
       "      <td>24.550</td>\n",
       "      <td>2017-01-01 00:10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>28.640</td>\n",
       "      <td>2017-01-01 00:10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>2</td>\n",
       "      <td>29.180</td>\n",
       "      <td>2017-01-01 00:10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>288</td>\n",
       "      <td>3</td>\n",
       "      <td>27.950</td>\n",
       "      <td>2017-01-01 00:10:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BUS COUNT  NUMBER OF READS  REGION_ID  SPEED                TIME\n",
       "0         12              215          5 25.230 2017-01-01 00:10:26\n",
       "1         12              230          6 24.550 2017-01-01 00:10:26\n",
       "2         10              150          1 28.640 2017-01-01 00:10:26\n",
       "3          6               85          2 29.180 2017-01-01 00:10:26\n",
       "4         20              288          3 27.950 2017-01-01 00:10:26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_df = pd.read_pickle('traffic_df.pkl')\n",
    "\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T23:41:56.995413Z",
     "start_time": "2019-03-06T23:41:51.550877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GabeKlick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#look through each unique region id\n",
    "for region in traffic_df.REGION_ID.unique():\n",
    "        \n",
    "        #create a dataframe with each unique region\n",
    "        globals()['region%s'%(region)] = region_df(region)\n",
    "        \n",
    "        #split data into train and test for modeling purposes\n",
    "        globals()['region%s_train'%(region)], globals()['region%s_test'%(region)] =\\\n",
    "        train_test_ts(globals()['region%s'%(region)])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,30):\n",
    "    \n",
    "    #create a model for each traffic region\n",
    "    ts_prophet, future, forecast, mean_forcast_bias, mean_MAPE, mean_RMSE =\\\n",
    "    fb_prophet_funct(globals()['region%s'%(i)])\n",
    "    \n",
    "    #rename model, metrics, and forecast\n",
    "    model_name = 'region%s_model'%(i)\n",
    "    metric_name = 'region%s_stats'%(i)\n",
    "    forecast_name = 'region%s_forecast'%(i)\n",
    "    \n",
    "    #pickle model, metrics, forecast\n",
    "    with open(model_name, \"wb\") as f:\n",
    "        pickle.dump(ts_prophet, f)\n",
    "    \n",
    "    with open(metric_name, \"wb\") as f:\n",
    "        pickle.dump([mean_forcast_bias, mean_MAPE, mean_RMSE],f)\n",
    "    \n",
    "    with open(forecast_name, \"wb\") as f:\n",
    "        pickle.dump(forecast,f)\n",
    "    \n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
